{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the material from the last class was new to me.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I didn't know a Wiener Process was Brownian Motion. \n",
    "\n",
    "A Wiener process, as mentioned in class, because Brown was studying pollen particles,  describes the random movement of particles in a space. \n",
    "\n",
    "**Definition and Properties:**\n",
    "\n",
    "A Wiener process, which is  $B(t)$ or $W(t)$  depending on the text, is a real-valued continuous-time stochastic process with the following properties:\n",
    "- Note that in the simple martingale world we were in discrete space, and now we have those properties, but are in continious space.\n",
    "\n",
    "\n",
    "1.  **$W(0) = 0$**: It starts at zero.\n",
    "2.  **Independent Increments**: For any $0 \\le s < t$, the increment $W(t) - W(s)$ is independent of the past values $W(u)$ for $0 \\le u \\le s$.\n",
    "3.  **Normally Distributed Increments**: For any $0 \\le s < t$, the increment $W(t) - W(s)$ is normally distributed with mean 0 and variance $t - s$, i.e.,\n",
    "    $W(t) - W(s) \\sim \\mathcal{N}(0, t - s)$\n",
    "4.  **Continuity**: $W(t)$ is almost surely continuous in $t$.\n",
    "\n",
    "**Key Implications:**\n",
    "\n",
    "* **Gaussian Process:** The Wiener process is a Gaussian process, meaning any finite collection of its values has a multivariate normal distribution.\n",
    "* **Mean and Variance:**\n",
    "    * The expected value of $W(t)$ is 0: $E[W(t)] = 0$.\n",
    "    * The variance of $W(t)$ is $t$: $Var[W(t)] = t$.\n",
    "* **Covariance:** The covariance between $W(s)$ and $W(t)$ is given by:\n",
    "    $Cov(W(s), W(t)) = E[W(s)W(t)] = \\min(s, t)$\n",
    "* **Non-differentiability**: Although continuous, the Wiener process is almost surely nowhere differentiable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking a step back, there are three intresting things here\n",
    "\n",
    "- **Covariance**  and it is $ \\min(s, t)$.... Why?\n",
    "- **Non-differentiability** \n",
    "- **Why is this a martingale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Covariance for Wiener Process\n",
    "\n",
    "**Assume s ≤ t, but assume the proof is the same if the reverse is true**\n",
    "\n",
    "If $s \\le t$, we can write $W(t)$ as:\n",
    "\n",
    "$W(t) = W(s) + (W(t) - W(s))$ .... Our add zero trick\n",
    "\n",
    "Now, with the tricks we have been using with expected value,  $W(s)W(t)$:\n",
    "\n",
    "$E[W(s)W(t)] = E[W(s)(W(s) + (W(t) - W(s)))]$\n",
    "\n",
    "Expanding this, we get:\n",
    "\n",
    "$E[W(s)W(t)] = E[W(s)^2 + W(s)(W(t) - W(s))]$\n",
    "\n",
    "$E[W(s)W(t)] = E[W(s)^2] + E[W(s)(W(t) - W(s))]$ .....Using the linearity of expectation\n",
    "\n",
    "Since $W(s)$ and $(W(t) - W(s))$ are independent, we have:\n",
    "\n",
    "$E[W(s)(W(t) - W(s))] = E[W(s)]E[W(t) - W(s)]$\n",
    "\n",
    "We know that $E[W(s)] = 0$ and $E[W(t) - W(s)] = 0$, so:\n",
    "\n",
    "$E[W(s)(W(t) - W(s))] = 0$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$E[W(s)W(t)] = E[W(s)^2]$\n",
    "\n",
    "$E[W(s)^2] = Var(W(s)) + (E[W(s)])^2 = s + 0^2 = s$.... since we know that $Var(W(s)) = s$, and since $E[W(s)] = 0$\n",
    "\n",
    "Thus, if $s \\le t$, then $E[W(s)W(t)] = s$.\n",
    "\n",
    "\n",
    "**Assume The same can be done for t ≤ s**\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "If you consider t ≤ s and s ≤ t:\n",
    "\n",
    "$E[W(s)W(t)] = \\min(s, t)$\n",
    "\n",
    "Since $E[W(s)] = 0$ and $E[W(t)] = 0$, the covariance is:\n",
    "\n",
    "$Cov(W(s), W(t)) = E[W(s)W(t)] - E[W(s)]E[W(t)] = E[W(s)W(t)] - 0 = \\min(s, t)$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$Cov(W(s), W(t)) = \\min(s, t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **What is the stochastic version of the chain rule?**  \n",
    "##### intresting: stochastic calculus is not differentialable.\n",
    "\n",
    "\n",
    "For a function $ f(t, X_t) $, where $ X_t $ follows a stochastic differential equation (SDE):  \n",
    "\n",
    "$\n",
    "dX_t = \\mu_t dt + \\sigma_t dW_t\n",
    "$\n",
    "\n",
    "where $ W_t $ is a **Brownian motion** , $ \\mu_t $ is the drift term, and $ \\sigma_t $ is the diffusion term, **Itô’s Lemma states that**:\n",
    "\n",
    "$\n",
    "df(t, X_t) = \\left( \\frac{\\partial f}{\\partial t} + \\mu_t \\frac{\\partial f}{\\partial x} + \\frac{1}{2} \\sigma_t^2 \\frac{\\partial^2 f}{\\partial x^2} \\right) dt + \\sigma_t \\frac{\\partial f}{\\partial x} dW_t\n",
    "$\n",
    "\n",
    "### 2. **Why is it used?**  \n",
    "\n",
    "**The tools from standard calculus do not work with stochastic processes**: Unlike ordinary differentiable functions, stochastic processes exhibit **quadratic variation**, meaning they have nonzero second-order infinitesimals (i.e., $ (dW_t)^2 = dt $). Itô’s formula correctly accounts for this effect.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Variation\n",
    "\n",
    "\n",
    "In class wer showed the relation of  Brownian motion to quadratic variation. In all honesty, I am not sure because it was the end of the day and I started to get out of it, but I was lost there. \n",
    "\n",
    "So here are some extra notes I took to supplement.\n",
    "\n",
    "Quadratic variation quantifies how much the process fluctuates over time. Quadratic variation tells us how much the squared changes of a point/particle accumulate over time. The fact that it grows linearly with time means that the particle's movement is highly erratic and doesn't smooth out over time.\n",
    "\n",
    "Basically we can use quadratic variation as a tool to measure the cumulative jumps, as it points to the extreme irregularity of its paths.\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "For a stochastic process $X(t)$ on the interval $[0, t]$, the quadratic variation is defined as the limit (in probability) of the sum of the squared increments of the process as the partition of the interval becomes finer.\n",
    "\n",
    "Let $0 = t_0 < t_1 < t_2 < ... < t_n = t$ be a partition of the interval $[0, t]$. The quadratic variation of $X(t)$ is:\n",
    "\n",
    "$[X, X]_t = \\lim_{n \\to \\infty} \\sum_{i=1}^n (X(t_i) - X(t_{i-1}))^2$\n",
    "\n",
    "where the limit is taken as $\\max_{1 \\le i \\le n} (t_i - t_{i-1}) \\to 0$.\n",
    "\n",
    "**Quadratic Variation of Brownian Motion:**\n",
    "\n",
    "For Brownian motion $W(t)$, the quadratic variation is:\n",
    "\n",
    "$[W, W]_t = t$\n",
    "\n",
    "This result is fundamental and signifies that the quadratic variation of Brownian motion grows linearly with time.\n",
    "\n",
    "**Why is Quadratic Variation Important for Brownian Motion?**\n",
    "\n",
    "1.  **Characterizing Roughness:** The quadratic variation is a way to quantify this extreme roughness. It tells us that the cumulative squared increments of Brownian motion grow linearly with time, indicating substantial fluctuations.\n",
    "\n",
    "2.  **Stochastic Calculus:**  It appears in the Itô lemma, which as stated above is basically the chain rule for stochastic calc. It's essential for defining stochastic integrals with respect to Brownian motion.\n",
    "\n",
    "3.  **Understanding Path Properties:** It helps in understanding the path properties of Brownian motion. For example, it explains why the paths have infinite total variation. The total variation of a function is the sum of the absolute differences of its values over a partition. Because brownian motion has a quadratic variation of t, the total variation is infinite.\n",
    "\n",
    "\n",
    "I am pretty excited to learn more about this in the coming semesters.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
