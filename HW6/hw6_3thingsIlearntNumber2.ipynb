{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Projection is intresting.\n",
    "\n",
    "\n",
    "And the building blocks of some important things.\n",
    "For example a unit vector.\n",
    "Our unit vector is simply U divided by the scaler magnitude of U.\n",
    "In some sense we are normalizing the vector.\n",
    "\n",
    "Now a projection is intresting.\n",
    "\n",
    "$\\operatorname{proj}_{\\mathbf{v}} \\mathbf{u} = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\| \\mathbf{v} \\|^2} \\mathbf{v}$\n",
    "\n",
    "So let's think about what is going on here\n",
    "\n",
    "\n",
    "$\\frac{\\mathbf{v}}{\\| \\mathbf{v} \\|} $ is the unit vector $\\mathbf{v}$\n",
    "\n",
    "$\\mathbf{u} \\cdot \\mathbf{v}$ creates a scaler, it is the dot product of two vectors, which gives a magnitude. But that might be a slight oversite. \n",
    "Remeber dot product is $|U||V| \\cos {\\theta}$. So their is direction involved.\n",
    "\n",
    "$\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\| \\mathbf{v} \\|} $ By dividing by the magnitude of $\\mathbf{v} $ we now normalize the scaler dot product in respect to $\\mathbf{v} $\n",
    "\n",
    "Another way of thinking about it is\n",
    "$\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\| \\mathbf{v} \\|} = \\frac{ |U||V| \\cos {\\theta}}{\\| \\mathbf{v} \\|} = |U|\\cos {\\theta}$\n",
    "\n",
    "\n",
    "So we have a unit vector of $\\mathbf{v} $ multiplied by a scaler dot product that is normalized to $\\mathbf{v} $ as well.\n",
    "\n",
    "So this will give us the proportion of the unit vector of $\\mathbf{v} $ .\n",
    "Maybe better said\n",
    "- unit vector scaled by the scalar projection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. What is a an egienvalue/vector?\n",
    "\n",
    "Is simply $Ax = \\lambda x$ Where $\\lambda$ is a diagonal matrix of values\n",
    "\n",
    "Eigenvectors make understanding linear transformations easier to understand, but I am not at quite there in terms of understanding.\n",
    "\n",
    "\n",
    "\n",
    "3. How are bivectors related to a determiment?\n",
    "\n",
    "\n",
    "Honestly I am not sure.\n",
    "I was reading about them in a text book.\n",
    "They were trying to describe determinants, but were introducing bivectors first.\n",
    "\n",
    "I can see that a bivector is\n",
    "\n",
    "$ u ∧ v$ \n",
    "\n",
    "How do I relate this to determinant?\n",
    "\n",
    "A = \n",
    "\\begin{bmatrix} \n",
    "a_{11} & a_{12} \\\\ \n",
    "a_{21} & a_{22} \n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "Obviously $a_{11} a_{22} - a_{21}a_{12}$\n",
    "\n",
    "\n",
    "I am getting lost with $(Ae_1) ∧(Ae_2) = c(Ae_1, Ae_2) e_1 ∧ e_2$\n",
    "\n",
    "I think $e_1$ is the basis vector or column vector of $A$, \n",
    "\n",
    "\n",
    "I want to understand this better because this leads to trivectors.\n",
    "\n",
    "Which is intresting to think about from a determinent persepective.\n",
    "\n",
    "\n",
    "4. Properties of determinents?\n",
    "\n",
    "\n",
    "- What is the determinent of a diagonal matrix? By diagnoal, I just mean everything zero, but the diagonal of a nxn matrix. It would be just multiplying the diagnoals together.\n",
    "\n",
    "- $det( A+B) \\neq det( A)+ det(B)$\n",
    "\n",
    "- swapping two rows of A changes the sign of det(A)\n",
    "\n",
    "- \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
