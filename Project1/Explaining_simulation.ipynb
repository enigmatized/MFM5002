{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are two different—yet complementary—ways to see why the variance estimate from my simulations converges to the “true” variance of the process being simulated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Large \\(N\\) and the Law of Large Numbers\n",
    "\n",
    "\n",
    "\n",
    "Wouldn't it be nice to just say, “running the simulation many times” converges on the correct variance.\"\n",
    "\n",
    "1. **Numerical experiments rely on random draws.** Each run of the simulation produces an outcome $ X_i $ .  \n",
    "2. **Variance in a random sample.** If you collect $N$ outcomes (i.e., run the simulation $N$ times), the sample variance $s^2$ is computed in the usual way:\n",
    "\n",
    "   $$\n",
    "   s^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} \\bigl(X_i - \\bar{X}\\bigr)^2,\n",
    "   \\quad\n",
    "   \\text{where}\n",
    "   \\quad\n",
    "   \\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_i.\n",
    "   $$\n",
    "\n",
    "3. **Law of Large Numbers for variance.** As \\(N\\) grows large, the sample variance $s^2$ will, *with high probability*, get closer and closer to the true variance  $\\sigma^2$. This is a direct consequence of the Law of Large Numbers.\n",
    "\n",
    "Basically, if we throw enough random draws into your estimate, the fluctuations shrink, and we get a stable estimate for the variance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on ways of attaining vairance via simulation\n",
    "\n",
    "##### Expressing the Sample Variance via Sample Moments\n",
    "\n",
    "Recall that the sample variance $s^2$ can be written in terms of the first and second sample moments:\n",
    "\n",
    "$$\n",
    "s^2\n",
    "\\;=\\;\n",
    "\\frac{1}{N - 1}\\sum_{i=1}^{N} \\bigl(X_i - \\bar{X}\\bigr)^2\n",
    "\\;=\\;\n",
    "\\bigl(\\overline{X^2}\\bigr) \\;-\\; \\bigl(\\bar{X}\\bigr)^2\n",
    "\\quad\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\bar{X}\n",
    "=\n",
    "\\frac{1}{N}\\sum_{i=1}^N X_i,\n",
    "\\quad\n",
    "\\overline{X^2}\n",
    "=\n",
    "\\frac{1}{N}\\sum_{i=1}^N X_i^2.\n",
    "$$\n",
    "\n",
    "The distribution and convergence of $\\bar{X}$ and $\\overline{X^2}$\n",
    "is key to the convergence of $s^2$.\n",
    "\n",
    "---\n",
    "\n",
    "Something I failed on the final at, and I need a richer understanding....\n",
    "\n",
    "#### CLT for Means and Second Moments\n",
    "\n",
    "#### CLT for the Sample Mean\n",
    "\n",
    "The CLT states that if\n",
    "$X_1, X_2, \\dots, X_N$\n",
    "are i.i.d. with mean $\\mu$ and variance $\\sigma^2$, then for large $N$,\n",
    "\n",
    "$$\n",
    "\\sqrt{N}\\,\\Bigl(\\bar{X} - \\mu\\Bigr)\n",
    "\\;\\xrightarrow{d}\\;\n",
    "\\mathcal{N}(0,\\,\\sigma^2),\n",
    "$$\n",
    "\n",
    "This equates to, $\\bar{X}$ is approximately normally distributed around $\\mu$, with a standard deviation shrinking like $1/\\sqrt{N}$.\n",
    "\n",
    "### CLT for the Sample Second Moment\n",
    "\n",
    "Similarly, if the underlying $X_i$ have a finite fourth moment, one can apply the CLT to $X_i^2$. \n",
    "\n",
    "$\\mu_2 = \\mathbb{E}[X^2]$. \n",
    "\n",
    "\n",
    "$$\n",
    "\\sqrt{N}\\,\\Bigl(\\overline{X^2} - \\mu_2\\Bigr)\n",
    "\\;\\xrightarrow{d}\\;\n",
    "\\mathcal{N}\\!\\bigl(0,\\,\\mathrm{Var}(X^2)\\bigr).\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Which basically means $\\overline{X^2}$  clusters around the true second moment $\\mu_2$ for large $N$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Putting It All Together\n",
    "\n",
    "Because\n",
    "\n",
    "$$\n",
    "s^2 \\;\\approx\\; \\overline{X^2} - (\\bar{X})^2,\n",
    "$$\n",
    "\n",
    "We can think of these as two random variables, $\\overline{X^2}$ and $\\bar{X}$, each obeying its own CLT. \n",
    "\n",
    "$$\n",
    "\\sqrt{N}\\,\\bigl(s^2 - \\sigma^2\\bigr)\n",
    "\\;\\xrightarrow{d}\\;\n",
    "\\text{some normal distribution}.\n",
    "$$\n",
    "\n",
    "Basically the CLT tells us that both $\\bar{X}$ and $\\overline{X^2}$ varies normally around their true means $\\mu$ and $ \\mu_2$ . The sample variance $s^2$ is just a combination of these, so *it, too*, converges on $\\sigma^2$ (the true variance) and fluctuates normally for large $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way one : 21273.95718171061\n",
      "way two : 21273.957181710968\n",
      "way three : 21273.957181710975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 10_000_000\n",
    "# Generate random values and assign 0.25 or 0.75 accordingly\n",
    "random_values = np.random.uniform(0, 1, size)\n",
    "result_vector = np.where(random_values < 0.25, 0.25, 0.75)\n",
    "\n",
    "# Create an array of -100 (make sure it has the same length as result_vector)\n",
    "array = np.full(size, -100)\n",
    "\n",
    "# Create the game1Vector as defined\n",
    "game1Vector = np.random.randint(1, 7, size) ** np.random.binomial(4, 0.5, size)\n",
    "\n",
    "# Compute the final vector:\n",
    "# For indices where result_vector is 0.25, multiply by array (-100)\n",
    "# For indices where result_vector is 0.75, multiply by game1Vector\n",
    "final_vector = np.where(result_vector == 0.25,\n",
    "                        array,\n",
    "                        game1Vector)\n",
    "\n",
    "\n",
    "sample_mean_squared = (np.mean(final_vector))**2\n",
    "\n",
    "sample_variance =  (final_vector ** 2) - sample_mean_squared\n",
    "\n",
    "print(f\"way one : {np.mean(sample_variance)}\")\n",
    "print(f\"way two : {np.var(final_vector)}\")\n",
    "print(f\"way three : {np.mean(final_vector ** 2) - sample_mean_squared}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on batching (averaging variances over subsets)\n",
    "\n",
    "Another technique I did in simulation is *batch means*.\n",
    "\n",
    "1. **Run simulations in batches.** Instead of running one massive block of $N$ simulation, break the runs into $k$ smaller blocks, each with $M = \\tfrac{N}{k}$ simulations.\n",
    "2. **Compute variance within each batch.** For each batch $b$, compute its sample variance $\\hat{\\sigma}_b^2$.\n",
    "\n",
    "\n",
    "3. **Average batch variances.** Then combine these $k$ variances by taking an average:\n",
    "\n",
    "   $$\n",
    "   \\hat{\\sigma}_{\\mathrm{avg}}^2 \n",
    "   \\;=\\; \n",
    "   \\frac{1}{k} \\sum_{b=1}^k \\hat{\\sigma}_b^2.\n",
    "   $$\n",
    "\n",
    "4. **Convergence of the average.** As $k$ and  $M$  gets large, get a stable estimate of the variance. This is neat, becase each batch variance is an independent (or nearly independent) estimate, averaging them smooths out short-term fluctuations.\n",
    "\n",
    "This method can be helpful:\n",
    "- It can be used in a way to estimate the variance of variance estimate (i.e., you can look at the spread of the $\\hat{\\sigma}_b^2$ values to get a confidence interval).\n",
    "\n",
    "\n",
    "### In conclusion\n",
    "\n",
    "- **First perspective (large $N$)**: Just keep simulating until sample variance converges to the true variance via the law of large numbers.\n",
    "- **Second perspective (batching)**: Subdivide large sample into independent batches, compute a variance in each batch, and then combine these estimates. This approach can often yield *faster, more robust* evidence that variance estimates are converging and lets ability for the variability of the variance itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HackathonJan2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
